# Pro Max Dual-Model System Implementation Guide

## æ¦‚è¿°

Pro Max ç³»ç»Ÿé€šè¿‡å¹¶è¡Œè°ƒç”¨ standard å’Œ pro æ¨¡å‹ï¼Œå®æ—¶æ¯”è¾ƒè´¨é‡å¹¶è‡ªåŠ¨é€‰æ‹©æœ€ä½³å“åº”ã€‚

## å·²å®Œæˆç»„ä»¶

### âœ… 1. proMaxManager.ts (385è¡Œ)

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
- å¹¶è¡Œæ¨¡å‹è°ƒç”¨
- Tokençº§æµå¼ä¼ è¾“
- è´¨é‡è¯„åˆ†ç®—æ³•ï¼ˆ0-100åˆ†ï¼‰
- è‡ªåŠ¨é€‰æ‹©æœ€ä½³å“åº”

**è´¨é‡è¯„åˆ†å› å­**ï¼š
- é•¿åº¦å› å­ï¼š50-500è¯æœ€ä½³
- ç»“æ„å› å­ï¼šåˆ—è¡¨ã€æ ‡é¢˜åŠ åˆ†
- å¼•ç”¨å› å­ï¼šæœ‰å¼•ç”¨åŠ 10åˆ†
- æ€è€ƒtokensï¼šProæ¨¡å‹ä¼˜åŠ¿ï¼ˆ+15åˆ†ï¼‰
- å¤šæ ·æ€§ï¼šé¿å…é‡å¤ï¼ˆrepetitionæƒ©ç½š-20åˆ†ï¼‰
- å®Œæ•´æ€§ï¼šæˆªæ–­æƒ©ç½š-15åˆ†

**ä¸»è¦API**ï¼š
```typescript
const manager = new ProMaxManager(standardConfig, proConfig)

await manager.invokeDual(
  { prompt, systemPrompt, temperature, maxTokens },
  {
    onStandardToken: (token, fullText) => { /* é€tokenå¤„ç† */ },
    onProToken: (token, fullText) => { /* é€tokenå¤„ç† */ },
    onBothComplete: (result) => {
      // result.selected = 'standard' | 'pro'
      // result.confidenceScore = å·®å¼‚åˆ†æ•°
    }
  },
  sseConnection  // å¯é€‰SSEè¿æ¥
)
```

## é›†æˆæ­¥éª¤

### Step 1: åˆå§‹åŒ–Pro Max Manager

åœ¨ `src/index.ts` å¯åŠ¨æ—¶ï¼š

```typescript
// 1. æ·»åŠ å¯¼å…¥
import {
  initializeProMaxManager,
  getProMaxManager,
} from './proMaxManager'

// 2. åœ¨logStartup()ä¸­åˆå§‹åŒ–
function logStartup(protocol: 'http' | 'https') {
  // ... ç°æœ‰ä»£ç  ...

  // ğŸ¤– Initialize Pro Max Manager
  initializeProMaxManager(
    aiConfig.standard,  // æ ‡å‡†æ¨¡å‹é…ç½®
    aiConfig.pro        // Proæ¨¡å‹é…ç½®
  )
  console.log('[pro-max] Dual-model system ready')
}
```

### Step 2: åˆ›å»ºPro Maxç«¯ç‚¹

åœ¨ `src/index.ts` æ·»åŠ æ–°endpointï¼ˆçº¦åœ¨5000è¡Œåï¼‰ï¼š

```typescript
/**
 * Pro Max Chat Endpoint
 * Invokes both standard and pro models in parallel
 */
app.post('/api/chat/pro-max', chatLimiter, async (req, res) => {
  const requestId = Math.random().toString(36).slice(2)
  console.log(`[pro-max] ${requestId} start`)

  if (!requireAuthenticated(req, res)) return

  try {
    const manager = getProMaxManager()
    if (!manager) {
      res.status(503).json({ ok: false, message: 'Pro Max not initialized' })
      return
    }

    // è§£æè¯·æ±‚
    const {
      message,
      language = 'zh',
      systemPrompt,
      temperature,
      maxTokens,
    } = req.body

    if (typeof message !== 'string' || !message.trim()) {
      res.status(400).json({ ok: false, message: 'Invalid message' })
      return
    }

    // æ£€æŸ¥æ˜¯å¦SSE
    const acceptSSE = req.headers.accept?.includes('text/event-stream')

    if (acceptSSE) {
      // SSEæ¨¡å¼ï¼šå®æ—¶æµå¼å“åº”
      res.setHeader('Content-Type', 'text/event-stream')
      res.setHeader('Cache-Control', 'no-cache')
      res.setHeader('Connection', 'keep-alive')

      const sseConnection = globalSSEPool.createConnection(res, requestId)

      const state = {
        standard: { text: '', complete: false },
        pro: { text: '', complete: false },
      }

      await manager.invokeDual(
        {
          prompt: message,
          systemPrompt,
          temperature,
          maxTokens,
        },
        {
          onStandardToken: (token, fullText) => {
            state.standard.text = fullText
            sseConnection.send({
              type: 'token',
              data: { model: 'standard', token, fullText },
            })
          },
          onProToken: (token, fullText) => {
            state.pro.text = fullText
            sseConnection.send({
              type: 'token',
              data: { model: 'pro', token, fullText },
            })
          },
          onBothComplete: (result) => {
            sseConnection.send({
              type: 'done',
              data: {
                standard: result.standard,
                pro: result.pro,
                selected: result.selected,
                confidence: result.confidenceScore,
              },
            })
            sseConnection.close()
          },
          onError: (error, model) => {
            sseConnection.sendError(error.message, { model })
          },
        },
        sseConnection
      )
    } else {
      // æ ‡å‡†æ¨¡å¼ï¼šç­‰å¾…å®Œæˆåè¿”å›
      const result = await manager.invokeDual(
        {
          prompt: message,
          systemPrompt,
          temperature,
          maxTokens,
        },
        {
          // éSSEæ¨¡å¼ä¸éœ€è¦tokenå¤„ç†
        }
      )

      res.json({
        ok: true,
        data: {
          standard: {
            text: result.standard.text,
            tokens: result.standard.tokens,
            durationMs: result.standard.durationMs,
          },
          pro: {
            text: result.pro.text,
            tokens: result.pro.tokens,
            durationMs: result.pro.durationMs,
          },
          selected: result.selected,
          confidence: result.confidenceScore,
        },
      })
    }
  } catch (error) {
    console.error(`[pro-max] ${requestId} error:`, error)
    res.status(500).json({
      ok: false,
      message: error instanceof Error ? error.message : 'Internal error',
    })
  }
})
```

### Step 3: å‰ç«¯é›†æˆï¼ˆå¯é€‰ï¼‰

åˆ›å»º `src/hooks/useProMaxChat.ts`ï¼š

```typescript
import { useState, useCallback } from 'react'
import { sendSSEChatMessage } from '../utils/sseClient'

export interface ProMaxResponse {
  standard: { text: string; tokens: number; durationMs: number }
  pro: { text: string; tokens: number; durationMs: number }
  selected: 'standard' | 'pro'
  confidence: number
}

export function useProMaxChat() {
  const [standardText, setStandardText] = useState('')
  const [proText, setProText] = useState('')
  const [result, setResult] = useState<ProMaxResponse | null>(null)
  const [loading, setLoading] = useState(false)

  const sendProMaxMessage = useCallback(async (message: string) => {
    setLoading(true)
    setStandardText('')
    setProText('')
    setResult(null)

    try {
      const client = await sendSSEChatMessage(
        '/api/chat/pro-max',
        { message },
        {
          onToken: (token, metadata) => {
            if (metadata.model === 'standard') {
              setStandardText(metadata.fullText)
            } else if (metadata.model === 'pro') {
              setProText(metadata.fullText)
            }
          },
          onDone: (finalData) => {
            setResult(finalData)
            setLoading(false)
          },
          onError: (error) => {
            console.error('[pro-max] Error:', error)
            setLoading(false)
          },
        }
      )
    } catch (error) {
      console.error('[pro-max] Failed to start:', error)
      setLoading(false)
    }
  }, [])

  return {
    standardText,
    proText,
    result,
    loading,
    sendProMaxMessage,
  }
}
```

åˆ›å»º `src/components/ProMaxChatPanel.tsx`ï¼š

```typescript
import { useState } from 'react'
import { useProMaxChat } from '../hooks/useProMaxChat'
import './ProMaxChatPanel.css'

export function ProMaxChatPanel() {
  const [input, setInput] = useState('')
  const { standardText, proText, result, loading, sendProMaxMessage } = useProMaxChat()

  const handleSend = () => {
    if (!input.trim() || loading) return
    sendProMaxMessage(input)
    setInput('')
  }

  return (
    <div className="pro-max-panel">
      <h2>Pro Max åŒæ¨¡å‹å¯¹æ¯”</h2>

      <div className="pro-max-input">
        <textarea
          value={input}
          onChange={(e) => setInput(e.target.value)}
          placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜..."
          disabled={loading}
        />
        <button onClick={handleSend} disabled={loading || !input.trim()}>
          {loading ? 'ç”Ÿæˆä¸­...' : 'å‘é€'}
        </button>
      </div>

      <div className="pro-max-results">
        {/* Standard æ¨¡å‹ */}
        <div className="model-response">
          <h3>Standard æ¨¡å‹</h3>
          <div className="response-text">
            {standardText || (loading ? 'ç”Ÿæˆä¸­...' : 'ç­‰å¾…è¾“å…¥')}
          </div>
          {result && (
            <div className="response-meta">
              <span>{result.standard.tokens} tokens</span>
              <span>{result.standard.durationMs}ms</span>
            </div>
          )}
        </div>

        {/* Pro æ¨¡å‹ */}
        <div className="model-response">
          <h3>Pro æ¨¡å‹</h3>
          <div className="response-text">
            {proText || (loading ? 'ç”Ÿæˆä¸­...' : 'ç­‰å¾…è¾“å…¥')}
          </div>
          {result && (
            <div className="response-meta">
              <span>{result.pro.tokens} tokens</span>
              <span>{result.pro.durationMs}ms</span>
            </div>
          )}
        </div>
      </div>

      {/* è‡ªåŠ¨é€‰æ‹©ç»“æœ */}
      {result && (
        <div className={`auto-selection auto-selection--${result.selected}`}>
          <strong>è‡ªåŠ¨é€‰æ‹©ï¼š</strong>
          {result.selected === 'standard' ? 'Standard æ¨¡å‹' : 'Pro æ¨¡å‹'}
          <span>ï¼ˆç½®ä¿¡åº¦ï¼š{result.confidence}ï¼‰</span>
        </div>
      )}
    </div>
  )
}
```

## æµ‹è¯•æ–¹æ³•

### 1. å‘½ä»¤è¡Œæµ‹è¯•

```bash
# éSSEæ¨¡å¼
curl -X POST http://localhost:4000/api/chat/pro-max \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "message": "è§£é‡Šä»€ä¹ˆæ˜¯é‡å­çº ç¼ ",
    "language": "zh"
  }'

# å“åº”ç¤ºä¾‹ï¼š
{
  "ok": true,
  "data": {
    "standard": {
      "text": "é‡å­çº ç¼ æ˜¯...",
      "tokens": 156,
      "durationMs": 2341
    },
    "pro": {
      "text": "é‡å­çº ç¼ ï¼ˆQuantum Entanglementï¼‰æ˜¯...",
      "tokens": 287,
      "durationMs": 4102
    },
    "selected": "pro",
    "confidence": 23
  }
}
```

### 2. SSEæ¨¡å¼æµ‹è¯•

```bash
curl -X POST http://localhost:4000/api/chat/pro-max \
  -H "Content-Type: application/json" \
  -H "Accept: text/event-stream" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "message": "ä»€ä¹ˆæ˜¯é»‘æ´",
    "language": "zh"
  }'

# SSEäº‹ä»¶æµï¼š
data: {"type":"token","data":{"model":"standard","token":"é»‘","fullText":"é»‘"}}
data: {"type":"token","data":{"model":"pro","token":"é»‘","fullText":"é»‘"}}
data: {"type":"token","data":{"model":"standard","token":"æ´","fullText":"é»‘æ´"}}
...
data: {"type":"done","data":{"selected":"pro","confidence":18}}
```

## é…ç½®é€‰é¡¹

åœ¨ `.env` æ·»åŠ Pro Maxé…ç½®ï¼š

```bash
# Pro Max å¯ç”¨å¼€å…³
ENABLE_PRO_MAX=true

# è‡ªåŠ¨é€‰æ‹©é˜ˆå€¼ï¼ˆç½®ä¿¡åº¦ä½äºæ­¤å€¼æ—¶ä½¿ç”¨Proå“åº”ï¼‰
PRO_MAX_AUTO_SELECT_THRESHOLD=15

# è¶…æ—¶è®¾ç½®
PRO_MAX_TIMEOUT_MS=120000  # 2åˆ†é’Ÿ
```

## æ€§èƒ½ä¼˜åŒ–

1. **å¹¶è¡Œæ‰§è¡Œ**ï¼šä¸¤ä¸ªæ¨¡å‹çœŸæ­£å¹¶è¡Œï¼Œæ— ç­‰å¾…
2. **æ—©åœæœºåˆ¶**ï¼šä¸€ä¸ªæ¨¡å‹å®Œæˆåç«‹å³æ˜¾ç¤ºï¼Œæ— éœ€ç­‰å¾…å¦ä¸€ä¸ª
3. **æµå¼ä¼ è¾“**ï¼šTokençº§å®æ—¶æ›´æ–°ï¼Œç”¨æˆ·ä½“éªŒæµç•…
4. **æ™ºèƒ½ç¼“å­˜**ï¼šç›¸åŒpromptå¯å¤ç”¨ç»“æœï¼ˆå¯é€‰å®ç°ï¼‰

## æ•…éšœå¤„ç†

- **å•æ¨¡å‹å¤±è´¥**ï¼šç»§ç»­ä½¿ç”¨æˆåŠŸçš„æ¨¡å‹å“åº”
- **åŒæ¨¡å‹å¤±è´¥**ï¼šè¿”å›å‹å¥½é”™è¯¯ä¿¡æ¯
- **è¶…æ—¶å¤„ç†**ï¼šä½¿ç”¨å·²ç”Ÿæˆçš„éƒ¨åˆ†å†…å®¹

## æœªæ¥æ‰©å±•

1. **ä¸‰æ¨¡å‹å¯¹æ¯”**ï¼šStandard + Pro + Pro-Maxï¼ˆThinkingæ¨¡å¼ï¼‰
2. **ç”¨æˆ·æŠ•ç¥¨**ï¼šè®©ç”¨æˆ·é€‰æ‹©æ›´å¥½çš„å“åº”ï¼Œè®­ç»ƒé€‰æ‹©ç®—æ³•
3. **A/Bæµ‹è¯•**ï¼šæ”¶é›†è´¨é‡è¯„åˆ†æ•°æ®ä¼˜åŒ–ç®—æ³•
4. **æ¨¡å‹è·¯ç”±**ï¼šæ ¹æ®é—®é¢˜ç±»å‹æ™ºèƒ½é€‰æ‹©è°ƒç”¨æ¨¡å¼
