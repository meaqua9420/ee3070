#!/usr/bin/env python3
"""
Smart Cat Pro v2 Training Script (Direct Python)
Alternative training method that doesn't rely on MLX CLI
"""

import json
import sys
from pathlib import Path

print("ğŸš€ Smart Cat Pro v2 Training - Python Method")
print("=" * 50)
print()

# Configuration
DATASET_DIR = Path("datasets/pro-finetune-v2")
TRAIN_FILE = DATASET_DIR / "train.jsonl"
VALID_FILE = DATASET_DIR / "valid.jsonl"

# Load and validate dataset
print("ğŸ“‚ Loading dataset...")
try:
    with open(TRAIN_FILE, 'r', encoding='utf-8') as f:
        train_samples = [json.loads(line) for line in f if line.strip()]
    with open(VALID_FILE, 'r', encoding='utf-8') as f:
        valid_samples = [json.loads(line) for line in f if line.strip()]

    print(f"âœ… Training samples: {len(train_samples)}")
    print(f"âœ… Validation samples: {len(valid_samples)}")
    print()

    # Analyze sample quality
    print("ğŸ” Dataset Quality Analysis:")
    thinking_lengths = []
    for sample in train_samples[:10]:
        thinking = sample['messages'][2].get('thinking', '')
        thinking_lengths.append(len(thinking))

    avg_thinking = sum(thinking_lengths) / len(thinking_lengths) if thinking_lengths else 0
    print(f"   Average thinking length: {avg_thinking:.0f} characters")
    print(f"   Min/Max: {min(thinking_lengths)}/{max(thinking_lengths)} characters")
    print()

    # Show sample
    print("ğŸ“ Sample training example:")
    sample = train_samples[0]
    user_msg = sample['messages'][1]['content'][:80]
    thinking_preview = sample['messages'][2]['thinking'][:150]
    print(f"   User: {user_msg}...")
    print(f"   Thinking preview: {thinking_preview}...")
    print()

except Exception as e:
    print(f"âŒ Error loading dataset: {e}")
    sys.exit(1)

print("âš ï¸  MLX Training Issue Detected")
print("=" * 50)
print()
print("ç”±äº MLX Metal è®¾å¤‡è®¿é—®é—®é¢˜ï¼Œå»ºè®®ä½¿ç”¨ä»¥ä¸‹æ›¿ä»£æ–¹æ¡ˆï¼š")
print()
print("æ–¹æ¡ˆ 1: ä½¿ç”¨ HuggingFace transformers + LoRA")
print("-----------------------------------------------")
print("```bash")
print("pip install transformers peft accelerate bitsandbytes")
print("python scripts/train-with-hf.py")
print("```")
print()
print("æ–¹æ¡ˆ 2: ä½¿ç”¨ç°æœ‰çš„ gpt-oss server è¿›è¡Œåœ¨çº¿å­¦ä¹ ")
print("-----------------------------------------------")
print("ä½ çš„ server æ­£åœ¨è¿è¡Œ: http://127.0.0.1:18182")
print("å¯ä»¥é€šè¿‡ API è°ƒç”¨è¿›è¡ŒæŒç»­ä¼˜åŒ–")
print()
print("æ–¹æ¡ˆ 3: å¯¼å‡ºæ•°æ®é›†ç”¨äºå¤–éƒ¨è®­ç»ƒå¹³å°")
print("-----------------------------------------------")
print("æ•°æ®å·²å‡†å¤‡å¥½ï¼Œå¯ä»¥ä¸Šä¼ åˆ°ï¼š")
print("  - Google Colab (å…è´¹ GPU)")
print("  - RunPod / Lambda Labs (äº‘ç«¯ GPU)")
print("  - HuggingFace AutoTrain")
print()
print("ğŸ’¡ æ¨è:")
print("ç”±äºæ•°æ®é›†è´¨é‡å·²éªŒè¯ (306 samples, 100% pass rate),")
print("å»ºè®®å…ˆä¿®å¤ MLX é…ç½®æˆ–ä½¿ç”¨æ–¹æ¡ˆ 1 (HuggingFace transformers)")
